{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81652895",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bdbf77",
   "metadata": {},
   "source": [
    "# IMPORTANT!!\n",
    "\n",
    "## note that for both sparse and EOT, function names are same, so we run the experiments sequentially, thus overwriting function definitions.\n",
    "\n",
    "## 1) experiments with end of trajectory feedback setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85e61b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    def __init__(self,k,d,size=8,danger=[7,1],goal=[4,5],wall=[2,5],coins=[(1,6),(4,2),(5,5)], horizon=50, noise=0.1):\n",
    "        self.noise = noise\n",
    "        self.k=k\n",
    "        self.d=d\n",
    "        self.size = size\n",
    "        self.horizon = horizon\n",
    "        self.goal = tuple(goal)\n",
    "        self.danger = tuple(danger)\n",
    "        self.wall = tuple(wall)\n",
    "        self._init_coins = tuple(map(tuple, coins)) \n",
    "        self.coins = set(self._init_coins)\n",
    "        self.collected_coins = set()\n",
    "        self.done = 0\n",
    "    \n",
    "    def reset(self):\n",
    "        self.done = 0\n",
    "        self.pos = (0,7)\n",
    "        self.t = 0\n",
    "        self.collected = 0\n",
    "        self.collected_coins = set()\n",
    "        self.coins = set(self._init_coins)\n",
    "        return self.pos\n",
    "    \n",
    "    def step(self, intended_action):\n",
    "        probs = np.full(4, 0.03)\n",
    "        probs[intended_action] = 0.91\n",
    "        action = np.random.choice(4, p=probs)\n",
    "        x, y = self.pos\n",
    "        if action == 0: x = max(0, x-1)       # up\n",
    "        if action == 1: x = min(self.size-1, x+1) # down\n",
    "        if action == 2: y = max(0, y-1)       # left\n",
    "        if action == 3: y = min(self.size-1, y+1) # right\n",
    "        if((x,y)!=(self.wall)):\n",
    "            self.pos = (x,y)\n",
    "        if self.pos in self.coins:\n",
    "            self.collected += 1\n",
    "            self.collected_coins.add(self.pos)\n",
    "            self.coins.remove(self.pos)\n",
    "        self.t += 1\n",
    "        self.done = ((self.t >= self.horizon)or (self.pos==self.goal) or (self.pos==self.danger))\n",
    "        return self.pos, self.done\n",
    "        \n",
    "\n",
    "    def get_feedback_and_features(self):\n",
    "        weights = [0.1, 1.0, 2.0, 3.0]\n",
    "        true_reward = (1-(self.pos==self.danger))*(weights[self.collected]*(self.collected + 1.32*(self.pos==self.goal)) - 5*(self.t-14)/self.horizon + 5*36/50)\n",
    "#         scaled_reward = 10*(1-math.exp(-true_reward/5))/(1+math.exp(-true_reward/5))\n",
    "        scaled_reward = 2*true_reward\n",
    "        ## now we quantize it into k bins\n",
    "        edges = np.linspace(0,32.92,self.k+1)\n",
    "        feedback = self.k-1\n",
    "        for i in range(len(edges)-1):\n",
    "            if(edges[i]<=scaled_reward and scaled_reward<edges[i+1]):\n",
    "                feedback = i\n",
    "                break\n",
    "                \n",
    "        # label noise\n",
    "        probs = [0.0] * self.k\n",
    "        probs[feedback] = 1-self.noise+self.noise/self.k\n",
    "        rem = 1-probs[feedback]\n",
    "        rem_distributed = rem / (self.k - 1)\n",
    "        for i in range(self.k):\n",
    "            if probs[i] == 0.0:\n",
    "                probs[i] = rem_distributed\n",
    "   \n",
    "        feedback_list = [i for i in range(self.k)]\n",
    "        feedback_given = np.random.choice(feedback_list,p=probs)\n",
    "        return feedback_given,self._features()\n",
    "    \n",
    "    def true_return(self):\n",
    "        weights = [0.1, 1.0, 2.0, 3.0]\n",
    "        true_reward = (1-(self.pos==self.danger))*(weights[self.collected]*(self.collected + 1.32*(self.pos==self.goal)) - 5*(self.t-14)/self.horizon + 5*36/50)\n",
    "        scaled_reward = 2*true_reward\n",
    "        edges = np.linspace(0,32.92,self.k+1)\n",
    "        feedback = self.k-1\n",
    "        for i in range(len(edges)-1):\n",
    "            if(edges[i]<=scaled_reward and scaled_reward<edges[i+1]):\n",
    "                feedback = i\n",
    "                break\n",
    "        return feedback\n",
    "                \n",
    "    \n",
    "    def _features(self):\n",
    "        \"\"\"return trajectory features phi(tau)\"\"\"\n",
    "        x, y = self.pos\n",
    "        xg, yg = self.goal\n",
    "        xd, yd = self.danger\n",
    "        dist_to_goal = abs(x-xg) + abs(y-yg)\n",
    "        dist_to_danger = abs(x-xd) + abs(y-yd)\n",
    "        at_danger = int(self.pos == self.danger)\n",
    "        at_goal = int(self.pos == self.goal and (at_danger==0))\n",
    "        coin_indicator = [int(c in self.collected_coins) for c in self._init_coins]\n",
    "        return np.array([dist_to_goal, dist_to_danger, at_goal, at_danger] + coin_indicator, dtype=float)\n",
    "        \n",
    "\n",
    "def softmax(logits):\n",
    "    exps = np.exp(logits - np.max(logits))\n",
    "    return exps / np.sum(exps)\n",
    "\n",
    "class Policy:\n",
    "    def __init__(self, grid_size, action_dim):\n",
    "        self.grid_size = grid_size\n",
    "        self.state_dim = grid_size * grid_size\n",
    "        self.action_dim = action_dim\n",
    "        self.theta = np.ones((self.state_dim, self.action_dim))\n",
    "    \n",
    "    def state_index(self, state):\n",
    "        return state[0] * self.grid_size + state[1]\n",
    "    \n",
    "    def act(self, state):\n",
    "        s_idx = self.state_index(state)\n",
    "        probs = softmax(self.theta[s_idx])\n",
    "        action = np.random.choice(len(probs), p=probs)\n",
    "        return action, probs\n",
    "    \n",
    "    def grad_log_prob(self, state, action):\n",
    "        \"\"\"Return (state_index, grad_row) with grad_row shape (action_dim,)\n",
    "           grad_row[j] = 1{j==action} - pi(j|s)\"\"\"\n",
    "        s_idx = self.state_index(state)\n",
    "        probs = softmax(self.theta[s_idx])\n",
    "        grad_row = -probs.copy()\n",
    "        grad_row[action] += 1.0\n",
    "        return s_idx, grad_row\n",
    "    \n",
    "\n",
    "\n",
    "class RewardModel:\n",
    "    def __init__(self, k,d,C=0.0):\n",
    "        self.k = k\n",
    "        self.d = d\n",
    "        self.W = np.zeros((k,d))\n",
    "        self.C = C\n",
    "        \n",
    "    def estimate_W(self, X, Y, reg=1e-3):\n",
    "        n, d = X.shape\n",
    "        W = cp.Variable((self.k, self.d))  # optimization variable, not vectorized, shape(k,d)\n",
    "        loss = 0\n",
    "        for i in range(n):\n",
    "            phi = X[i]  \n",
    "            yi = int(Y[i])\n",
    "            logits = W @ phi \n",
    "            ## yi cannot be more than (k-1), so if assigning deterministic rewards without crafting W*, be careful\n",
    "            loss += -(logits[yi] - cp.log_sum_exp(logits))\n",
    "        \n",
    "        loss = loss/n + reg*cp.norm(W, \"fro\")**2\n",
    "        prob = cp.Problem(cp.Minimize(loss))\n",
    "        prob.solve(solver=cp.MOSEK)\n",
    "\n",
    "        self.W = W.value\n",
    "        return self.W\n",
    "    \n",
    "    def reward_probabilities(self, phi):\n",
    "        \"\"\"estimate P(y|tau)\"\"\"\n",
    "        logits = self.W @ phi\n",
    "        logits-= np.max(logits)\n",
    "        exp_logits = np.exp(logits)\n",
    "        return exp_logits/np.sum(exp_logits)\n",
    "    \n",
    "    def reward_estimate(self,phi):\n",
    "        \"\"\"returns the average, expected reward given the reward probabilities\"\"\"\n",
    "        return np.sum(np.array([i*self.reward_probabilities(phi)[i] for i in range(self.k)]))\n",
    "    \n",
    "    def optimistic_reward(self, phi, n_samples):\n",
    "        \"\"\"\n",
    "        optimism term included\n",
    "        \"\"\"\n",
    "        base = self.reward_estimate(phi)\n",
    "        n = max(n_samples, 1)\n",
    "        bonus = self.C / np.sqrt(n)\n",
    "        optimistic = base + bonus\n",
    "        return min(optimistic, self.k - 1)\n",
    "\n",
    "\n",
    "###----------Traning loop-------------###\n",
    "def train(N=20,m=50,k=6,eta=0.1,epsilon=0.1,grid_size=8,danger=[7,1],goal=[0,7], wall=[2,5] ,horizon=50,coins=None,seed=0,noise=0.1):\n",
    "    np.random.seed(seed)\n",
    "    queries = 0\n",
    "    steps = 0\n",
    "    if coins is None:\n",
    "        coins=[(1,6),(4,2),(5,5)]\n",
    "    print(\"hi\")\n",
    "    d = 4+len(coins)\n",
    "#     W_true = generate_W_true(k,d)\n",
    "    env = GridWorld(k,d,size=grid_size, danger=danger, goal=goal, wall=wall, coins=coins, horizon=horizon, noise=noise)\n",
    "    policy = Policy(grid_size=grid_size, action_dim=4)\n",
    "    \n",
    "    ## initialize weights w_0\n",
    "    reward_model = RewardModel(k, d, C=10.0)\n",
    "    reward_model.W = np.zeros((k,d))\n",
    "    \n",
    "    all_data_X, all_data_Y = [], []\n",
    "    avg_true_rewards,avg_coins,avg_est_rewards = [], [], []\n",
    "    flag = 0\n",
    "    for n in range(N):\n",
    "        if flag: break\n",
    "        print(n)\n",
    "        avg_true_reward_this_iter = 0\n",
    "        avg_est_reward_this_iter = 0\n",
    "        if(n<100):\n",
    "            G_range = 10\n",
    "        elif (n>100 ):\n",
    "            G_range = 100\n",
    "\n",
    "        for g in range(G_range):\n",
    "            steps+=1\n",
    "            rollout_trajectories = []\n",
    "            avg_true_reward_this_iter = 0\n",
    "            avg_est_reward_this_iter = 0\n",
    "            rewards = []\n",
    "            for i in range(m): ## sample trajectories under current policy pi to approiximate the theoretical expectation\n",
    "                s = env.reset()\n",
    "                traj = {\"states\": [], \"actions\": [], \"steps\":0, \"coins\":0}\n",
    "                done = False\n",
    "\n",
    "                while not done:\n",
    "                    a, _ = policy.act(s)\n",
    "                    traj[\"states\"].append(s)\n",
    "                    traj[\"actions\"].append(a)\n",
    "                    s, done = env.step(a)\n",
    "            \n",
    "                traj[\"steps\"] = env.horizon\n",
    "                traj[\"coins\"] = env.collected\n",
    "                y, phi = env.get_feedback_and_features()\n",
    "                phi = np.array(phi, dtype=float)   \n",
    "                rollout_trajectories.append((traj, phi, y))\n",
    "                rewards.append(env.true_return())\n",
    "#             if(np.mean(rewards)>31):\n",
    "#                 flag = 1\n",
    "#                 break\n",
    "\n",
    "                \n",
    "            ## now with these m rollouts, approximate the expectation of estimated reward under policy pi\n",
    "            grad_theta = np.zeros_like(policy.theta)\n",
    "            n_samples = max(len(all_data_X), 1)\n",
    "            R_hats = [reward_model.optimistic_reward(phi, n_samples) for _, phi, _ in rollout_trajectories]\n",
    "            b = float(np.mean(R_hats))  # baseline\n",
    "                        \n",
    "            for (traj,phi_tau,y), r_hat in zip(rollout_trajectories,R_hats):\n",
    "                \n",
    "                avg_est_reward_this_iter+=r_hat/len(rollout_trajectories)\n",
    "#                 avg_true_reward_this_iter+=y/len(rollout_trajectories)\n",
    "                temp = r_hat-b\n",
    "                    \n",
    "                for state,action in zip(traj[\"states\"],traj[\"actions\"]):\n",
    "                    s_idx, grad_row = policy.grad_log_prob(state, action)\n",
    "                    grad_theta[s_idx] += grad_row*(temp)\n",
    "\n",
    "            grad_theta = grad_theta/len(rollout_trajectories)\n",
    "            policy.theta += eta*grad_theta\n",
    "            avg_true_reward_this_iter = np.mean(rewards)\n",
    "            \n",
    "        done = False\n",
    "        s = env.reset()\n",
    "        while not done:\n",
    "            a,_ = policy.act(s)\n",
    "            s,done = env.step(a)\n",
    "        y,phi = env.get_feedback_and_features()\n",
    "        all_data_X.append(phi)\n",
    "        all_data_Y.append(y)\n",
    "        reward_model.W = reward_model.estimate_W(np.array(all_data_X),np.array(all_data_Y), reg = 1e-3)\n",
    "         \n",
    "        traj,phi,y = rollout_trajectories[-1]\n",
    "\n",
    "        coins_this_iter = traj[\"coins\"]\n",
    "        \n",
    "        ## storing some info\n",
    "        avg_est_rewards.append(avg_est_reward_this_iter)\n",
    "        avg_true_rewards.append(avg_true_reward_this_iter)\n",
    "        avg_coins.append(coins_this_iter)\n",
    "        \n",
    "        # update estimate of weight matrix W\n",
    "        reward_model.W = reward_model.estimate_W(np.array(all_data_X),np.array(all_data_Y), reg = 1e-3)\n",
    "\n",
    "        print(f\"Iter {n:02d}: avg_estimated_reward={avg_est_rewards[-1]:.3f}, avg_true_reward={avg_true_rewards[-1]:.3f},coins_this_episode={avg_coins[-1]:.2f}\")\n",
    "    \n",
    "    return policy, reward_model, avg_true_rewards, avg_est_rewards, reward_model.W,steps,queries\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea703a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N_EPISODES = 200\n",
    "seeds = [0, 1, 2, 3, 4]\n",
    "\n",
    "all_curves_01 = []\n",
    "\n",
    "for seed in seeds:\n",
    "    policy, rm, avg_true, avg_est, W, steps, queries = train(\n",
    "        N=N_EPISODES,\n",
    "        m=20,\n",
    "        k=6,\n",
    "        eta=0.5,\n",
    "        grid_size=8,\n",
    "        danger=(7, 1),\n",
    "        goal=(4, 5),\n",
    "        wall=(2, 5),\n",
    "        coins=[(1, 6), (4, 2), (5, 5)],\n",
    "        seed=seed,\n",
    "        noise=0.1\n",
    "    )\n",
    "    all_curves_01.append(np.array(avg_true, dtype=float))\n",
    "\n",
    "all_curves_01 = np.stack(all_curves_01, axis=0)   \n",
    "\n",
    "mean_curve_01 = all_curves_01.mean(axis=0)\n",
    "std_curve_01  = all_curves_01.std(axis=0)\n",
    "\n",
    "x_01 = np.arange(N_EPISODES)\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(x, mean_curve_01, label=\"mean avg_true over seeds\")\n",
    "plt.fill_between(x, mean_curve_01 - std_curve_01, mean_curve_01 + std_curve_01,\n",
    "                 alpha=0.2, label=\"±1 std over seeds\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Average feedback per episode\")\n",
    "plt.title(\"avg feedback per episode averaged over seeds\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9005f226",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N_EPISODES = 200\n",
    "seeds = [0, 1, 2, 3, 4]\n",
    "\n",
    "all_curves_05 = []\n",
    "\n",
    "for seed in seeds:\n",
    "    policy, rm, avg_true, avg_est, W, steps, queries = train(\n",
    "        N=N_EPISODES,\n",
    "        m=20,\n",
    "        k=6,\n",
    "        eta=0.5,\n",
    "        grid_size=8,\n",
    "        danger=(7, 1),\n",
    "        goal=(4, 5),\n",
    "        wall=(2, 5),\n",
    "        coins=[(1, 6), (4, 2), (5, 5)],\n",
    "        seed=seed,\n",
    "        noise=0.5\n",
    "    )\n",
    "    all_curves_05.append(np.array(avg_true, dtype=float))\n",
    "\n",
    "all_curves_05 = np.stack(all_curves_05, axis=0)   \n",
    "\n",
    "mean_curve_05 = all_curves_05.mean(axis=0)\n",
    "std_curve_05  = all_curves_05.std(axis=0)\n",
    "\n",
    "x_05 = np.arange(N_EPISODES)\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(x_05, mean_curve_05, label=\"mean avg_true over seeds\")\n",
    "plt.fill_between(x_05, mean_curve_05 - std_curve_05, mean_curve_05 + std_curve_05,\n",
    "                 alpha=0.2, label=\"±1 std over seeds\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Average feedback per episode\")\n",
    "plt.title(\"avg feedback per episode averaged over seeds\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d57b9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N_EPISODES = 200\n",
    "seeds = [0, 1, 2, 3, 4]\n",
    "\n",
    "all_curves_08 = []\n",
    "\n",
    "for seed in seeds:\n",
    "    policy, rm, avg_true, avg_est, W, steps, queries = train(\n",
    "        N=N_EPISODES,\n",
    "        m=20,\n",
    "        k=6,\n",
    "        eta=0.5,\n",
    "        grid_size=8,\n",
    "        danger=(7, 1),\n",
    "        goal=(4, 5),\n",
    "        wall=(2, 5),\n",
    "        coins=[(1, 6), (4, 2), (5, 5)],\n",
    "        seed=seed,\n",
    "        noise=0.8\n",
    "    )\n",
    "    all_curves_08.append(np.array(avg_true, dtype=float))\n",
    "\n",
    "all_curves_08 = np.stack(all_curves_08, axis=0)   \n",
    "\n",
    "mean_curve_08 = all_curves_08.mean(axis=0)\n",
    "std_curve_08  = all_curves_08.std(axis=0)\n",
    "\n",
    "x_08 = np.arange(N_EPISODES)\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(x_08, mean_curve_08, label=\"mean avg_true over seeds\")\n",
    "plt.fill_between(x_08, mean_curve_08 - std_curve_08, mean_curve_08 + std_curve_08,\n",
    "                 alpha=0.2, label=\"±1 std over seeds\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Average feedback per episode\")\n",
    "plt.title(\"avg feedback per episode averaged over seeds\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdeb4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Stack and compute mean/std for each noise level\n",
    "all_curves_01 = np.stack(all_curves_01, axis=0)\n",
    "all_curves_05 = np.stack(all_curves_05, axis=0)\n",
    "all_curves_08 = np.stack(all_curves_08, axis=0)\n",
    "\n",
    "mean_01 = all_curves_01.mean(axis=0)\n",
    "std_01  = all_curves_01.std(axis=0)\n",
    "\n",
    "mean_05 = all_curves_05.mean(axis=0)\n",
    "std_05  = all_curves_05.std(axis=0)\n",
    "\n",
    "mean_08 = all_curves_08.mean(axis=0)\n",
    "std_08  = all_curves_08.std(axis=0)\n",
    "\n",
    "N_EPISODES = mean_01.shape[0]  \n",
    "x = np.arange(N_EPISODES)\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "\n",
    "# noise = 0.1\n",
    "plt.plot(x, mean_01, label=\"noise = 0.1\")\n",
    "plt.fill_between(x, mean_01 - std_01, mean_01 + std_01, alpha=0.15)\n",
    "\n",
    "# noise = 0.5\n",
    "plt.plot(x, mean_05, label=\"noise = 0.5\")\n",
    "plt.fill_between(x, mean_05 - std_05, mean_05 + std_05, alpha=0.15)\n",
    "\n",
    "# noise = 0.8\n",
    "plt.plot(x, mean_08, label=\"noise = 0.8\")\n",
    "plt.fill_between(x, mean_08 - std_08, mean_08 + std_08, alpha=0.15)\n",
    "\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Average feedback per episode\")\n",
    "plt.title(\"Average feedback per episode (mean ± 1 std over seeds)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbe7a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## optionally, save these for comparing algorithms later\n",
    "# import numpy as np\n",
    "\n",
    "# np.save('mean_01.npy', mean_01)\n",
    "# np.save('std_01.npy', std_01)\n",
    "# np.save('mean_05.npy', mean_05)\n",
    "# np.save('std_05.npy', std_05)\n",
    "# np.save('mean_08.npy', mean_08)\n",
    "# np.save('std_08.npy', std_08)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47803beb",
   "metadata": {},
   "source": [
    "## 2) experiments with sparse feedback setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a626f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    def __init__(self,k,d,size=8,danger=[7,1],goal=[4,5],wall=[2,5],\n",
    "                 coins=[(1,6),(4,2),(5,5)], horizon=50, noise=0.1):\n",
    "        self.noise = noise\n",
    "        self.k = k\n",
    "        self.d = d\n",
    "        self.size = size\n",
    "        self.horizon = horizon\n",
    "        self.goal = tuple(goal)\n",
    "        self.danger = tuple(danger)\n",
    "        self.wall = tuple(wall)\n",
    "        self._init_coins = tuple(map(tuple, coins)) \n",
    "        self.coins = set(self._init_coins)\n",
    "        self.collected_coins = set()\n",
    "        self.done = 0\n",
    "    \n",
    "    def reset(self):\n",
    "        self.done = 0\n",
    "        self.pos = (0,7)\n",
    "        self.t = 0\n",
    "        self.collected = 0\n",
    "        self.collected_coins = set()\n",
    "        self.coins = set(self._init_coins)\n",
    "        return self.pos\n",
    "    \n",
    "    def step(self, intended_action):\n",
    "        probs = np.full(4, 0.03)\n",
    "        probs[intended_action] = 0.91\n",
    "        action = np.random.choice(4, p=probs)\n",
    "        x, y = self.pos\n",
    "        if action == 0: x = max(0, x-1)       # up\n",
    "        if action == 1: x = min(self.size-1, x+1) # down\n",
    "        if action == 2: y = max(0, y-1)       # left\n",
    "        if action == 3: y = min(self.size-1, y+1) # right\n",
    "        if (x,y) != (self.wall):\n",
    "            self.pos = (x,y)\n",
    "        if self.pos in self.coins:\n",
    "            self.collected += 1\n",
    "            self.collected_coins.add(self.pos)\n",
    "            self.coins.remove(self.pos)\n",
    "        self.t += 1\n",
    "        self.done = ((self.t >= self.horizon) or \n",
    "                     (self.pos==self.goal) or \n",
    "                     (self.pos==self.danger))\n",
    "        return self.pos, self.done\n",
    "        \n",
    "\n",
    "    def get_feedback_and_features(self):\n",
    "        weights = [0.1, 1.0, 2.0, 3.0]\n",
    "        true_reward = (1-(self.pos==self.danger))*(\n",
    "            weights[self.collected]*(self.collected + 1.32*(self.pos==self.goal))\n",
    "            - 5*(self.t-14)/self.horizon + 5*36/50\n",
    "        )\n",
    "        # scaled_reward = 10*(1-math.exp(-true_reward/5))/(1+math.exp(-true_reward/5))\n",
    "        scaled_reward = 2*true_reward\n",
    "\n",
    "        # quantize into k bins\n",
    "        edges = np.linspace(0, 32.92, self.k+1)\n",
    "        feedback = self.k-1\n",
    "        for i in range(len(edges)-1):\n",
    "            if edges[i] <= scaled_reward < edges[i+1]:\n",
    "                feedback = i\n",
    "                break\n",
    "                \n",
    "        # label noise\n",
    "        probs = [0.0] * self.k\n",
    "        probs[feedback] = 1 - self.noise + self.noise/self.k\n",
    "        rem = 1 - probs[feedback]\n",
    "        rem_distributed = rem / (self.k - 1)\n",
    "        for i in range(self.k):\n",
    "            if probs[i] == 0.0:\n",
    "                probs[i] = rem_distributed\n",
    "   \n",
    "        feedback_list = [i for i in range(self.k)]\n",
    "        feedback_given = np.random.choice(feedback_list,p=probs)\n",
    "        return feedback_given, self._features()\n",
    "    \n",
    "    def true_return(self):\n",
    "        weights = [0.1, 1.0, 2.0, 3.0]\n",
    "        true_reward = (1-(self.pos==self.danger))*(\n",
    "            weights[self.collected]*(self.collected + 1.32*(self.pos==self.goal))\n",
    "            - 5*(self.t-14)/self.horizon + 5*36/50\n",
    "        )\n",
    "        scaled_reward = 2*true_reward\n",
    "        edges = np.linspace(0, 32.92, self.k+1)\n",
    "        feedback = self.k-1\n",
    "        for i in range(len(edges)-1):\n",
    "            if edges[i] <= scaled_reward < edges[i+1]:\n",
    "                feedback = i\n",
    "                break\n",
    "        return feedback\n",
    "                \n",
    "    \n",
    "    def _features(self):\n",
    "        \"\"\"return trajectory features phi(tau)\"\"\"\n",
    "        x, y = self.pos\n",
    "        xg, yg = self.goal\n",
    "        xd, yd = self.danger\n",
    "        dist_to_goal = abs(x-xg) + abs(y-yg)\n",
    "        dist_to_danger = abs(x-xd) + abs(y-yd)\n",
    "        at_danger = int(self.pos == self.danger)\n",
    "        at_goal = int(self.pos == self.goal and (at_danger==0))\n",
    "        coin_indicator = [int(c in self.collected_coins) for c in self._init_coins]\n",
    "        return np.array([dist_to_goal, dist_to_danger, at_goal, at_danger] + coin_indicator,\n",
    "                        dtype=float)\n",
    "        \n",
    "\n",
    "def softmax(logits):\n",
    "    exps = np.exp(logits - np.max(logits))\n",
    "    return exps / np.sum(exps)\n",
    "\n",
    "class Policy:\n",
    "    def __init__(self, grid_size, action_dim):\n",
    "        self.grid_size = grid_size\n",
    "        self.state_dim = grid_size * grid_size\n",
    "        self.action_dim = action_dim\n",
    "        self.theta = np.ones((self.state_dim, self.action_dim))\n",
    "    \n",
    "    def state_index(self, state):\n",
    "        return state[0] * self.grid_size + state[1]\n",
    "    \n",
    "    def act(self, state):\n",
    "        s_idx = self.state_index(state)\n",
    "        probs = softmax(self.theta[s_idx])\n",
    "        action = np.random.choice(len(probs), p=probs)\n",
    "        return action, probs\n",
    "    \n",
    "    def grad_log_prob(self, state, action):\n",
    "        \"\"\"Return (state_index, grad_row) with grad_row shape (action_dim,)\n",
    "           grad_row[j] = 1{j==action} - pi(j|s)\"\"\"\n",
    "        s_idx = self.state_index(state)\n",
    "        probs = softmax(self.theta[s_idx])\n",
    "        grad_row = -probs.copy()\n",
    "        grad_row[action] += 1.0\n",
    "        return s_idx, grad_row\n",
    "    \n",
    "\n",
    "\n",
    "class RewardModel:\n",
    "    def __init__(self, k,d,C=0.0):\n",
    "        self.k = k\n",
    "        self.d = d\n",
    "        self.W = np.zeros((k,d))\n",
    "        self.C = C\n",
    "        \n",
    "    def estimate_W(self, X, Y, reg=1e-3):\n",
    "        n, d = X.shape\n",
    "        W = cp.Variable((self.k, self.d))  # optimization variable, shape (k,d)\n",
    "        loss = 0\n",
    "        for i in range(n):\n",
    "            phi = X[i]  \n",
    "            yi = int(Y[i])\n",
    "            logits = W @ phi \n",
    "            loss += -(logits[yi] - cp.log_sum_exp(logits))\n",
    "        \n",
    "        loss = loss/n + reg*cp.norm(W, \"fro\")**2\n",
    "        prob = cp.Problem(cp.Minimize(loss))\n",
    "        prob.solve(solver=cp.MOSEK)\n",
    "\n",
    "        self.W = W.value\n",
    "        return self.W\n",
    "    \n",
    "    def reward_probabilities(self, phi):\n",
    "        \"\"\"estimate P(y|tau)\"\"\"\n",
    "        logits = self.W @ phi\n",
    "        logits -= np.max(logits)\n",
    "        exp_logits = np.exp(logits)\n",
    "        return exp_logits/np.sum(exp_logits)\n",
    "    \n",
    "    def reward_estimate(self,phi):\n",
    "        \"\"\"returns E[y | phi]\"\"\"\n",
    "        probs = self.reward_probabilities(phi)\n",
    "        return np.sum(np.arange(self.k) * probs)\n",
    "    \n",
    "    def optimistic_reward(self, phi, n_samples):\n",
    "        \"\"\"\n",
    "        optimism term included\n",
    "        \"\"\"\n",
    "        base = self.reward_estimate(phi)\n",
    "        n = max(n_samples, 1)\n",
    "        bonus = self.C / np.sqrt(n)\n",
    "        optimistic = base + bonus\n",
    "        return min(optimistic, self.k - 1)\n",
    "\n",
    "\n",
    "###----------Training loop with model-based multi-feedback-------------###\n",
    "def train(\n",
    "    N=20,\n",
    "    m=50,\n",
    "    k=6,\n",
    "    eta=0.1,\n",
    "    epsilon=0.1,         # unused, kept for API compatibility\n",
    "    grid_size=8,\n",
    "    danger=[7,1],\n",
    "    goal=[4,5],\n",
    "    wall=[2,5],\n",
    "    horizon=50,\n",
    "    coins=None,\n",
    "    seed=0,\n",
    "    noise=0.1,\n",
    "    feedback_every=10,   # NEW: how often to query feedback within a trajectory\n",
    "):\n",
    "    np.random.seed(seed)\n",
    "    queries = 0\n",
    "    steps = 0\n",
    "    if coins is None:\n",
    "        coins=[(1,6),(4,2),(5,5)]\n",
    "    print(\"hi\")\n",
    "    d = 4+len(coins)\n",
    "\n",
    "    env = GridWorld(k,d,size=grid_size, danger=danger, goal=goal,\n",
    "                    wall=wall, coins=coins, horizon=horizon, noise=noise)\n",
    "    policy = Policy(grid_size=grid_size, action_dim=4)\n",
    "    \n",
    "    ## initialize weights w_0\n",
    "    reward_model = RewardModel(k, d, C=10.0)\n",
    "    reward_model.W = np.zeros((k,d))\n",
    "    \n",
    "    all_data_X, all_data_Y = [], []\n",
    "    avg_true_rewards, avg_coins, avg_est_rewards = [], [], []\n",
    "    flag = 0\n",
    "\n",
    "    for n in range(N):              # <-- outer loop over episodes (unchanged)\n",
    "        if flag: break\n",
    "        print(n)\n",
    "        avg_est_reward_this_iter = 0.0\n",
    "\n",
    "        # same logic for how many gradient steps per episode\n",
    "        if n < 100:\n",
    "            G_range = 10\n",
    "        else:\n",
    "            G_range = 100\n",
    "\n",
    "        for g in range(G_range):    # <-- inner gradient-descent iterations\n",
    "            rollout_trajectories = []\n",
    "            # per-g stats\n",
    "            rewards = []\n",
    "            est = []\n",
    "            for i in range(m):  # sample m trajectories under current policy\n",
    "                s = env.reset()\n",
    "                traj = {\"states\": [], \"actions\": [], \"steps\": 0,\n",
    "                        \"coins\": 0, \"step_rewards\": []}\n",
    "                done = False\n",
    "\n",
    "                prev_y_hat = 0.0     # predicted cumulative label so far\n",
    "                last_y = 0.0         # last (noisy) label index for logging\n",
    "                last_y_hat = 0.0     # last predicted cumulative label\n",
    "\n",
    "                # number of samples used so far in RM training (for optimism)\n",
    "                n_samples = max(len(all_data_X), 1)\n",
    "\n",
    "                while not done:\n",
    "                    a, _ = policy.act(s)\n",
    "                    traj[\"states\"].append(s)\n",
    "                    traj[\"actions\"].append(a)\n",
    "                    s, done = env.step(a)\n",
    "                    steps += 1\n",
    "\n",
    "                    step_r = 0.0\n",
    "\n",
    "                    # MULTI-FEEDBACK: query at intervals or at episode end\n",
    "                    if (env.t % feedback_every == 0) or done:\n",
    "                        queries += 1\n",
    "                        y, phi = env.get_feedback_and_features()\n",
    "                        phi = np.array(phi, dtype=float)\n",
    "\n",
    "                        last_y = float(y)\n",
    "\n",
    "                        # model-based predicted cumulative rating\n",
    "                        y_hat = reward_model.optimistic_reward(phi, n_samples)\n",
    "                        last_y_hat = float(y_hat)\n",
    "\n",
    "                        # incremental reward = Δ y_hat\n",
    "                        step_r = y_hat - prev_y_hat\n",
    "                        prev_y_hat = y_hat\n",
    "\n",
    "                    traj[\"step_rewards\"].append(step_r)\n",
    "\n",
    "                traj[\"steps\"] = env.t\n",
    "                traj[\"coins\"] = env.collected\n",
    "\n",
    "                # logging: approximate \"true\" and \"estimated\" label from last feedback\n",
    "                est.append(reward_model.optimistic_reward(phi, n_samples))\n",
    "                rewards.append(env.true_return())\n",
    "\n",
    "                # compute reward-to-go for this trajectory (REINFORCE)\n",
    "                returns = []\n",
    "                G_t = 0.0\n",
    "                for r_step in reversed(traj[\"step_rewards\"]):\n",
    "                    G_t = r_step + G_t\n",
    "                    returns.insert(0, G_t)\n",
    "\n",
    "                rollout_trajectories.append((traj, returns))\n",
    "\n",
    "            # ----- policy gradient update for this g -----\n",
    "            grad_theta = np.zeros_like(policy.theta)\n",
    "\n",
    "            # baseline from all returns across all m trajectories\n",
    "            all_returns = [ret for _, rets in rollout_trajectories for ret in rets]\n",
    "            if len(all_returns) == 0:\n",
    "                b = 0.0\n",
    "            else:\n",
    "                b = float(np.mean(all_returns))\n",
    "\n",
    "            for traj, returns in rollout_trajectories:\n",
    "                for state, action, G_t in zip(traj[\"states\"],\n",
    "                                              traj[\"actions\"],\n",
    "                                              returns):\n",
    "                    s_idx, grad_row = policy.grad_log_prob(state, action)\n",
    "                    grad_theta[s_idx] += grad_row * (G_t - b)\n",
    "\n",
    "            grad_theta = grad_theta / len(rollout_trajectories)\n",
    "            policy.theta += eta * grad_theta\n",
    "        \n",
    "        avg_true_reward_this_iter = np.mean(rewards)\n",
    "        avg_est_reward_this_iter = np.mean(est)\n",
    "\n",
    "        # ===== end of outer episode n: collect ONE label for reward model =====\n",
    "        s = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            a, _ = policy.act(s)\n",
    "            s, done = env.step(a)\n",
    "            steps += 1\n",
    "        queries += 1\n",
    "        y_final, phi_final = env.get_feedback_and_features()\n",
    "        phi_final = np.array(phi_final, dtype=float)\n",
    "\n",
    "        all_data_X.append(phi_final)\n",
    "        all_data_Y.append(y_final)\n",
    "\n",
    "        # fit reward model on all collected episode-level samples\n",
    "        reward_model.W = reward_model.estimate_W(\n",
    "            np.array(all_data_X), np.array(all_data_Y), reg=1e-3\n",
    "        )\n",
    "\n",
    "        # take last trajectory of the last g for coins logging\n",
    "        last_traj, _ = rollout_trajectories[-1]\n",
    "        coins_this_iter = last_traj[\"coins\"]\n",
    "        \n",
    "        # store info for plotting\n",
    "        avg_est_rewards.append(avg_est_reward_this_iter)\n",
    "        avg_true_rewards.append(avg_true_reward_this_iter)\n",
    "        avg_coins.append(coins_this_iter)\n",
    "        \n",
    "        print(\n",
    "            f\"Iter {n:02d}: \"\n",
    "            f\"avg_estimated_label={avg_est_rewards[-1]:.3f}, \"\n",
    "            f\"avg_true_label={avg_true_rewards[-1]:.3f}, \"\n",
    "            f\"coins_this_episode={avg_coins[-1]:.2f}\"\n",
    "        )\n",
    "    \n",
    "    return policy, reward_model, avg_true_rewards, avg_est_rewards, reward_model.W, steps, queries\n",
    "\n",
    "\n",
    "# Example call (like yours, just with feedback_every if you want to change it)\n",
    "trained_policy, trained_reward_model, avg_true, avg_est, W, steps, queries = train(\n",
    "    N=200,\n",
    "    m=20,\n",
    "    k=6,\n",
    "    eta=0.5,\n",
    "    epsilon=1e-2,\n",
    "    grid_size=8,\n",
    "    danger=[7,1],\n",
    "    goal=[4,5],\n",
    "    wall=[2,5],\n",
    "    coins=[(1,6),(4,2),(5,5)],\n",
    "    seed=1,\n",
    "    noise=0.8,\n",
    "    feedback_every=10,   # you can sweep over this\n",
    ")\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78488996",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "N_EPISODES = 200         \n",
    "seeds = [0, 1, 2, 3, 4]   # seeds to average over\n",
    "\n",
    "feedback_every_list = [1, 10, 20]   # how often to query within a trajectory\n",
    "noise_levels = [0.1, 0.5, 0.8]      # label noise in env\n",
    "\n",
    "M = 20               \n",
    "K = 6              \n",
    "ETA = 0.5            \n",
    "GRID_SIZE = 8\n",
    "DANGER = [7, 1]\n",
    "GOAL   = [4, 5]\n",
    "WALL   = [2, 5]\n",
    "COINS  = [(1, 6), (4, 2), (5, 5)]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for fb_every in feedback_every_list:\n",
    "    for noise in noise_levels:\n",
    "        print(f\"\\n=== feedback_every={fb_every}, noise={noise} ===\")\n",
    "        curves = []\n",
    "\n",
    "        for seed in seeds:\n",
    "            print(f\"  Seed {seed}\")\n",
    "            policy, rm, avg_true, avg_est, W, steps, queries = train(\n",
    "                N=N_EPISODES,\n",
    "                m=M,\n",
    "                k=K,\n",
    "                eta=ETA,\n",
    "                grid_size=GRID_SIZE,\n",
    "                danger=DANGER,\n",
    "                goal=GOAL,\n",
    "                wall=WALL,\n",
    "                coins=COINS,\n",
    "                seed=seed,\n",
    "                noise=noise,\n",
    "                feedback_every=fb_every,\n",
    "            )\n",
    "            curves.append(np.array(avg_true, dtype=float))\n",
    "\n",
    "        curves = np.stack(curves, axis=0)   # shape: [num_seeds, N_EPISODES]\n",
    "        mean_curve = curves.mean(axis=0)\n",
    "        std_curve  = curves.std(axis=0)\n",
    "\n",
    "        results[(fb_every, noise)] = {\n",
    "            \"mean\": mean_curve,\n",
    "            \"std\": std_curve,\n",
    "        }\n",
    "\n",
    "x = np.arange(N_EPISODES)\n",
    "\n",
    "for noise in noise_levels:\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    for fb_every in feedback_every_list:\n",
    "        key = (fb_every, noise)\n",
    "        mean_curve = results[key][\"mean\"]\n",
    "        std_curve  = results[key][\"std\"]\n",
    "\n",
    "        label = f\"fb_every={fb_every}\"\n",
    "        plt.plot(x, mean_curve, label=label)\n",
    "        plt.fill_between(\n",
    "            x,\n",
    "            mean_curve - std_curve,\n",
    "            mean_curve + std_curve,\n",
    "            alpha=0.2,\n",
    "        )\n",
    "\n",
    "    plt.xlabel(\"Episode (outer iter n)\")\n",
    "    plt.ylabel(\"Average feedback per episode (avg_true)\")\n",
    "    plt.title(f\"avg feedback per episode (noise={noise})\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "for fb_every in feedback_every_list:\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    for noise in noise_levels:\n",
    "        key = (fb_every, noise)\n",
    "        mean_curve = results[key][\"mean\"]\n",
    "        std_curve  = results[key][\"std\"]\n",
    "\n",
    "        label = f\"noise={noise}\"\n",
    "        plt.plot(x, mean_curve, label=label)\n",
    "        plt.fill_between(\n",
    "            x,\n",
    "            mean_curve - std_curve,\n",
    "            mean_curve + std_curve,\n",
    "            alpha=0.2,\n",
    "        )\n",
    "\n",
    "    plt.xlabel(\"Episode (outer iter n)\")\n",
    "    plt.ylabel(\"Average feedback per episode (avg_true)\")\n",
    "    plt.title(f\"avg feedback per episode (feedback_every={fb_every})\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c008079",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# # --- load end-of-trajectory curves ---\n",
    "# mean_01 = np.load('mean_01.npy')\n",
    "# std_01  = np.load('std_01.npy')\n",
    "# mean_05 = np.load('mean_05.npy')\n",
    "# std_05  = np.load('std_05.npy')\n",
    "# mean_08 = np.load('mean_08.npy')\n",
    "# std_08  = np.load('std_08.npy')\n",
    "\n",
    "episodes = np.arange(len(mean_01))\n",
    "\n",
    "def get_fb20(noise):\n",
    "    return results[(20, noise)][\"mean\"], results[(20, noise)][\"std\"]\n",
    "\n",
    "mean_fb20_01, std_fb20_01 = get_fb20(0.1)\n",
    "mean_fb20_05, std_fb20_05 = get_fb20(0.5)\n",
    "mean_fb20_08, std_fb20_08 = get_fb20(0.8)\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "\n",
    "# noise = 0.1\n",
    "line01, = plt.plot(episodes, mean_01, label='end-only, noise = 0.1')\n",
    "c01 = line01.get_color()\n",
    "plt.fill_between(episodes, mean_01 - std_01, mean_01 + std_01, alpha=0.2, color=c01)\n",
    "\n",
    "plt.plot(episodes, mean_fb20_01, '--', color=c01,\n",
    "         label='sparse fb=20, noise = 0.1')\n",
    "plt.fill_between(episodes, mean_fb20_01 - std_fb20_01,\n",
    "                 mean_fb20_01 + std_fb20_01, alpha=0.15, color=c01)\n",
    "\n",
    "# noise = 0.5\n",
    "line05, = plt.plot(episodes, mean_05, label='end-only, noise = 0.5')\n",
    "c05 = line05.get_color()\n",
    "plt.fill_between(episodes, mean_05 - std_05, mean_05 + std_05, alpha=0.2, color=c05)\n",
    "\n",
    "plt.plot(episodes, mean_fb20_05, '--', color=c05,\n",
    "         label='sparse fb=20, noise = 0.5')\n",
    "plt.fill_between(episodes, mean_fb20_05 - std_fb20_05,\n",
    "                 mean_fb20_05 + std_fb20_05, alpha=0.15, color=c05)\n",
    "\n",
    "# noise = 0.8\n",
    "line08, = plt.plot(episodes, mean_08, label='end-only, noise = 0.8')\n",
    "c08 = line08.get_color()\n",
    "plt.fill_between(episodes, mean_08 - std_08, mean_08 + std_08, alpha=0.2, color=c08)\n",
    "\n",
    "plt.plot(episodes, mean_fb20_08, '--', color=c08,\n",
    "         label='sparse fb=20, noise = 0.8')\n",
    "plt.fill_between(episodes, mean_fb20_08 - std_fb20_08,\n",
    "                 mean_fb20_08 + std_fb20_08, alpha=0.15, color=c08)\n",
    "\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Average feedback per episode\")\n",
    "plt.title(\"Average feedback per episode: end-of-trajectory vs sparse (fb_every=20)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3120749b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "episodes = np.arange(len(mean_01))\n",
    "\n",
    "\n",
    "def get_fb10(noise):\n",
    "    return results[(10, noise)][\"mean\"], results[(10, noise)][\"std\"]\n",
    "\n",
    "mean_fb10_01, std_fb10_01 = get_fb10(0.1)\n",
    "mean_fb10_05, std_fb10_05 = get_fb10(0.5)\n",
    "mean_fb10_08, std_fb10_08 = get_fb10(0.8)\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "\n",
    "# noise = 0.1\n",
    "line01, = plt.plot(episodes, mean_01, label='end-only, noise = 0.1')\n",
    "c01 = line01.get_color()\n",
    "plt.fill_between(episodes, mean_01 - std_01, mean_01 + std_01, alpha=0.2, color=c01)\n",
    "\n",
    "plt.plot(episodes, mean_fb10_01, '--', color=c01,\n",
    "         label='sparse fb=20, noise = 0.1')\n",
    "plt.fill_between(episodes, mean_fb10_01 - std_fb10_01,\n",
    "                 mean_fb10_01 + std_fb10_01, alpha=0.15, color=c01)\n",
    "\n",
    "# noise = 0.5\n",
    "line05, = plt.plot(episodes, mean_05, label='end-only, noise = 0.5')\n",
    "c05 = line05.get_color()\n",
    "plt.fill_between(episodes, mean_05 - std_05, mean_05 + std_05, alpha=0.2, color=c05)\n",
    "\n",
    "plt.plot(episodes, mean_fb10_05, '--', color=c05,\n",
    "         label='sparse fb=20, noise = 0.5')\n",
    "plt.fill_between(episodes, mean_fb10_05 - std_fb10_05,\n",
    "                 mean_fb10_05 + std_fb10_05, alpha=0.15, color=c05)\n",
    "\n",
    "# noise = 0.8\n",
    "line08, = plt.plot(episodes, mean_08, label='end-only, noise = 0.8')\n",
    "c08 = line08.get_color()\n",
    "plt.fill_between(episodes, mean_08 - std_08, mean_08 + std_08, alpha=0.2, color=c08)\n",
    "\n",
    "plt.plot(episodes, mean_fb10_08, '--', color=c08,\n",
    "         label='sparse fb=20, noise = 0.8')\n",
    "plt.fill_between(episodes, mean_fb10_08 - std_fb10_08,\n",
    "                 mean_fb10_08 + std_fb10_08, alpha=0.15, color=c08)\n",
    "\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Average feedback per episode\")\n",
    "plt.title(\"Average feedback per episode: end-of-trajectory vs sparse (fb_every=20)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65aa63ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "episodes = np.arange(len(mean_01))\n",
    "\n",
    "def get_fb1(noise):\n",
    "    return results[(1, noise)][\"mean\"], results[(1, noise)][\"std\"]\n",
    "\n",
    "mean_fb1_01, std_fb1_01 = get_fb1(0.1)\n",
    "mean_fb1_05, std_fb1_05 = get_fb1(0.5)\n",
    "mean_fb1_08, std_fb1_08 = get_fb1(0.8)\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "\n",
    "# noise = 0.1\n",
    "line01, = plt.plot(episodes, mean_01, label='end-only, noise = 0.1')\n",
    "c01 = line01.get_color()\n",
    "plt.fill_between(episodes, mean_01 - std_01, mean_01 + std_01, alpha=0.2, color=c01)\n",
    "\n",
    "plt.plot(episodes, mean_fb1_01, '--', color=c01,\n",
    "         label='sparse fb=20, noise = 0.1')\n",
    "plt.fill_between(episodes, mean_fb1_01 - std_fb1_01,\n",
    "                 mean_fb1_01 + std_fb1_01, alpha=0.15, color=c01)\n",
    "\n",
    "# noise = 0.5\n",
    "line05, = plt.plot(episodes, mean_05, label='end-only, noise = 0.5')\n",
    "c05 = line05.get_color()\n",
    "plt.fill_between(episodes, mean_05 - std_05, mean_05 + std_05, alpha=0.2, color=c05)\n",
    "\n",
    "plt.plot(episodes, mean_fb1_05, '--', color=c05,\n",
    "         label='sparse fb=20, noise = 0.5')\n",
    "plt.fill_between(episodes, mean_fb1_05 - std_fb1_05,\n",
    "                 mean_fb1_05 + std_fb1_05, alpha=0.15, color=c05)\n",
    "\n",
    "# noise = 0.8\n",
    "line08, = plt.plot(episodes, mean_08, label='end-only, noise = 0.8')\n",
    "c08 = line08.get_color()\n",
    "plt.fill_between(episodes, mean_08 - std_08, mean_08 + std_08, alpha=0.2, color=c08)\n",
    "\n",
    "plt.plot(episodes, mean_fb1_08, '--', color=c08,\n",
    "         label='sparse fb=20, noise = 0.8')\n",
    "plt.fill_between(episodes, mean_fb1_08 - std_fb1_08,\n",
    "                 mean_fb1_08 + std_fb1_08, alpha=0.15, color=c08)\n",
    "\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Average feedback per episode\")\n",
    "plt.title(\"Average feedback per episode: end-of-trajectory vs sparse (fb_every=20)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
